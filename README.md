# Yai 프로젝트
학습 과정 

KoBERT(혹은 일반 BERT) 계열 모델은 “입력 문장 → 토큰화 → Transformer 인코더 → 최종 벡터(예: [CLS] 토큰의 임베딩) → 분류 헤드(Classifier)” 같은 단계를 통해 카테고리를 학습합니다.

아래는 일반적인 흐름을 간단히 정리한 예시입니다(모델 구조나 구현에 따라 조금씩 다를 수 있지만, 기본 개념은 비슷합니다).

1) 토큰화(Tokenization)
CSV에 있는 각 문장(예: “숙소가 깨끗하고 서비스가 좋았습니다.”)을 BERT 토크나이저(여기서는 KoBERT 토크나이저)를 사용해 토큰으로 나눕니다.
예: ["[CLS]", "숙소", "가", "깨끗", "##하", "고", "서", "##비", "##스", "가", "좋", "##았", "##습", "##니다", ".", "[SEP]"]
그 뒤 이 토큰들을 모델이 이해할 수 있는 정수 인덱스(input_ids)로 바꿉니다.
예: [2, 1234, 678, 3456, 789, ... 3] (토크나이저에 따라 실제 숫자는 다릅니다.)
동시에, attention_mask(패딩 구간을 구별해주기 위해 0/1로 구성), token_type_ids(세그먼트 정보, 문장 구분용) 등도 생성됩니다.
2) Transformer 인코더(BERT 본체, KoBERT)
입력된 토큰 인덱스 시퀀스는 KoBERT의 Transformer 인코더를 거칩니다.
각 토큰마다 임베딩 벡터가 계산되고, 여러 층(Layer)의 self-attention/FeedForward 과정을 거쳐 최종적으로 문맥 정보를 풍부하게 담은 벡터들이 생성됩니다.
보통 분류를 할 때는, 맨 앞 [CLS] 토큰의 최종 벡터를 사용하거나, 특정 풀링(pooling) 방식으로 전체 시퀀스 정보를 요약해 하나의 벡터를 뽑아냅니다.
3) 분류 헤드(Classification Head)
[CLS] 토큰 벡터를 입력으로 하는 작은 분류기(FC Layer 등) 를 붙입니다.
예: 
Linear
(
Hidden Dimension
→
Number of Categories
)
Linear(Hidden Dimension→Number of Categories)
예를 들어, “경치(0), 서비스(1), 청결(2), …” 이런 식으로 0~N개의 카테고리가 있다면, 분류기는 최종적으로 N차원 로짓(각 카테고리별 점수)을 출력합니다.
훈련(학습)할 때는, CSV에 있는 정답 레이블(0,1,2,3,4,...)과 모델이 예측한 로짓을 비교하여 크로스 엔트로피 오차(cross-entropy loss) 를 계산하고, 이를 역전파로 최소화하면서 가중치를 업데이트합니다.
4) 학습(Training) 과정
Forward
문장 입력 → KoBERT를 통과해 [CLS] 임베딩 추출 → 최종 분류 Layer → 예측값(logits) 생성
Loss 계산
예측값과 CSV에서 가져온 정답 라벨(예: 2 = 청결)을 비교해 크로스 엔트로피 오차를 계산
Backward + Optimizer
오차를 줄이도록 KoBERT(Transformer)와 분류 Layer의 가중치를 역전파(backpropagation)로 업데이트
이 과정을 반복(에폭-Epoch) 하면서, 모델이 “입력 문장 → 해당 카테고리” 사이의 관계를 점점 잘 학습하게 됩니다.

5) 정리
**CSV 파일에 문장과 ‘카테고리 레이블’**이 있으면, KoBERT는 그 문장을 토크나이징해서 입력받고, Transformer 인코더로 문맥 임베딩을 얻은 뒤, 최종 분류 레이어에서 각 카테고리 점수를 출력합니다.
학습 단계에서 정답 레이블과의 차이를 이용해 오차를 역전파함으로써, “문장의 특징”과 “카테고리” 사이의 관계를 최적화합니다.
그렇게 학습이 끝나면, 새로운 문장이 들어왔을 때 가장 점수가 높은 카테고리를 예측할 수 있게 됩니다.
즉, “문장이 있고, 그 문장에 대응하는 정답 라벨(카테고리 번호)”이 주어져 있을 때, BERT 계열 모델(여기서는 KoBERT)은 문장 → 벡터 표현으로 만든 뒤, 가장 그럴듯한 카테고리를 예측하도록 학습을 진행한다고 보시면 됩니다.
